{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.current_index = 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return tokens\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.word_index:\n",
    "                    self.word_index[token] = self.current_index\n",
    "                    self.index_word[self.current_index] = token\n",
    "                    self.current_index += 1\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            sequence = [self.word_index[token] for token in tokens if token in self.word_index]\n",
    "            sequences.append(sequence)\n",
    "        return sequences\n",
    "\n",
    "    def sequences_to_texts(self, sequences):\n",
    "        texts = []\n",
    "        for sequence in sequences:\n",
    "            text = ' '.join([self.index_word[index] for index in sequence])\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(Tokenizer().fit_on_texts([\"I love machine learning\", \"Machine learning is fun\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.total_counts = defaultdict(int)\n",
    "\n",
    "    def train(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            for i in range(len(sequence) - 1):\n",
    "                current_word = sequence[i]\n",
    "                next_word = sequence[i + 1]\n",
    "                self.bigram_counts[current_word][next_word] += 1\n",
    "                self.total_counts[current_word] += 1\n",
    "\n",
    "    def predict_next_word(self, current_word):\n",
    "        if current_word not in self.bigram_counts:\n",
    "            return None\n",
    "        next_words = self.bigram_counts[current_word]\n",
    "        return max(next_words, key=next_words.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next word after 'i' is 'love'\n"
     ]
    }
   ],
   "source": [
    "# 示例文本\n",
    "texts = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"I love coding\",\n",
    "    \"Coding is great\"\n",
    "]\n",
    "\n",
    "# 初始化Tokenizer并拟合文本\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 将文本转换为序列\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# 初始化语言模型并训练\n",
    "model = BigramLanguageModel()\n",
    "model.train(sequences)\n",
    "\n",
    "# 测试模型\n",
    "current_word = tokenizer.word_index['i']\n",
    "next_word_index = model.predict_next_word(current_word)\n",
    "next_word = tokenizer.index_word[next_word_index]\n",
    "\n",
    "print(f\"The next word after 'i' is '{next_word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.total_counts = defaultdict(int)\n",
    "\n",
    "    def train(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            for i in range(len(sequence) - 1):\n",
    "                current_word = sequence[i]\n",
    "                next_word = sequence[i + 1]\n",
    "                self.bigram_counts[current_word][next_word] += 1\n",
    "                self.total_counts[current_word] += 1\n",
    "\n",
    "    def predict_next_word(self, current_word):\n",
    "        if current_word not in self.bigram_counts:\n",
    "            return None\n",
    "        next_words = self.bigram_counts[current_word]\n",
    "        return max(next_words, key=next_words.get)\n",
    "\n",
    "    def generate_text(self, start_word, max_length=10):\n",
    "        current_word = start_word\n",
    "        text = [current_word]\n",
    "        for _ in range(max_length - 1):\n",
    "            next_word_index = self.predict_next_word(current_word)\n",
    "            if next_word_index is None:\n",
    "                break\n",
    "            next_word = next_word_index\n",
    "            text.append(next_word)\n",
    "            current_word = next_word\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I\n",
      "Bot: i love machine learning is fun genshin impact\n"
     ]
    }
   ],
   "source": [
    "def chat(model, tokenizer, input_text, max_length=10):\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    sequences = tokenizer.texts_to_sequences([input_text])\n",
    "    if not sequences[0]:\n",
    "        return \"Sorry, I didn't understand that.\"\n",
    "    start_word = sequences[0][-1]\n",
    "    generated_sequence = model.generate_text(start_word, max_length)\n",
    "    response = tokenizer.sequences_to_texts([generated_sequence])\n",
    "    return response[0]\n",
    "\n",
    "texts = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"fun genshin impact\",\n",
    "    \"I love coding\",\n",
    "    \"Coding is great\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model.train(sequences)\n",
    "\n",
    "user_input = \"I\"\n",
    "response = chat(model, tokenizer, user_input)\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
